{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57c43426",
   "metadata": {},
   "source": [
    "## Importing necessary libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7c1f923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a575b60f",
   "metadata": {},
   "source": [
    "## Cleaning and Encoding Name Strings for Model Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dff3cdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"names.xlsx\")\n",
    "df.rename(columns={'Michael': 'Names'}, inplace=True)\n",
    "names_list = df[\"Names\"].dropna().astype(str).tolist()\n",
    "names_list = ['.' + names + '.' for names in names_list]\n",
    "chars = sorted(list(set(''.join(names_list))))\n",
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:ch for ch, i in stoi.items()}\n",
    "encoded_names = [[stoi[ch] for ch in name] for name in names_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f774297d",
   "metadata": {},
   "source": [
    "## Generating Fixed-Length Sequences and Targets from Encoded Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98d606c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Input: [0, 0, 0, 0, 0], Target: 3\n",
      "1: Input: [0, 0, 0, 0, 3], Target: 34\n",
      "2: Input: [0, 0, 0, 3, 34], Target: 44\n",
      "3: Input: [0, 0, 3, 34, 44], Target: 35\n",
      "4: Input: [0, 3, 34, 44, 35], Target: 45\n",
      "5: Input: [3, 34, 44, 35, 45], Target: 46\n",
      "6: Input: [34, 44, 35, 45, 46], Target: 41\n",
      "7: Input: [44, 35, 45, 46, 41], Target: 42\n",
      "8: Input: [35, 45, 46, 41, 42], Target: 34\n",
      "9: Input: [45, 46, 41, 42, 34], Target: 31\n"
     ]
    }
   ],
   "source": [
    "seq_length = 5\n",
    "X, y = [], []\n",
    "for name in encoded_names:\n",
    "    for i in range(1, len(name)):\n",
    "        start = max(0, i - seq_length)\n",
    "        seq = name[start:i] \n",
    "        seq = [0] * (seq_length - len(seq)) + seq\n",
    "        X.append(seq)\n",
    "        y.append(name[i])\n",
    "        \n",
    "for i in range(len(encoded_names))[:10]:\n",
    "    print(f\"{i}: Input: {X[i]}, Target: {y[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163d29e4",
   "metadata": {},
   "source": [
    "## Splitting Data into Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa3a32a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.8 * len(X))\n",
    "X_train, X_test = X[:n], X[n:]\n",
    "y_train, y_test = y[:n], y[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a90d79",
   "metadata": {},
   "source": [
    "## Initializing Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c73e2e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(chars)\n",
    "hidden_size = 100\n",
    "embedding_size = 32\n",
    "\n",
    "Wxh1 = torch.randn(hidden_size, embedding_size) * 0.01\n",
    "Whh1 = torch.randn(hidden_size, hidden_size) * 0.01\n",
    "bh1  = torch.zeros(hidden_size, 1)\n",
    "\n",
    "Wxh2 = torch.randn(hidden_size, hidden_size) * 0.01  \n",
    "Whh2 = torch.randn(hidden_size, hidden_size) * 0.01 \n",
    "bh2  = torch.zeros(hidden_size, 1)\n",
    "\n",
    "Why = torch.randn(vocab_size, hidden_size) * 0.01\n",
    "by  = torch.zeros(vocab_size, 1)\n",
    "\n",
    "Embedding_vector = torch.randn(vocab_size, embedding_size)\n",
    "\n",
    "bgain1 = torch.ones((1, hidden_size))\n",
    "bnbias1 = torch.zeros((1, hidden_size))\n",
    "\n",
    "bgain2 = torch.ones((1, hidden_size))\n",
    "bnbias2 = torch.zeros((1, hidden_size))\n",
    "\n",
    "parameters = [Wxh1, Whh1, bh1, Wxh2, Whh2, bh2, Why, by, Embedding_vector, bgain1, bnbias1, bgain2, bnbias2]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e5042b",
   "metadata": {},
   "source": [
    "## Model Functions â€“ Forward, Backward, and Loss Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ad60a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    x = torch.clamp(x, -20, 20)\n",
    "    e_pos = torch.exp(x)\n",
    "    e_neg = torch.exp(-x)\n",
    "    return (e_pos - e_neg) / (e_pos + e_neg)\n",
    "\n",
    "def softmax_cross_entropy(out_batch, target_batch):\n",
    "    out_batch = out_batch - torch.max(out_batch, dim=1, keepdim=True)[0]\n",
    "    exp_out = torch.exp(out_batch)\n",
    "    probs = exp_out / exp_out.sum(dim=1, keepdim=True)\n",
    "    \n",
    "    correct_probs = probs[torch.arange(len(target_batch)), target_batch]\n",
    "    loss = -torch.log(correct_probs).mean()\n",
    "    return loss, probs\n",
    "\n",
    "def batch_normalization(h, bgain=None, bnbias=None, eps=1e-5):\n",
    "    bnmean = torch.mean(h, dim=0, keepdim=True)   \n",
    "    bnstd  = torch.std(h, dim=0, keepdim=True)       \n",
    "    h_norm = (h - bnmean) / (bnstd + eps)         \n",
    "    h_out = bgain * h_norm + bnbias                \n",
    "    return h_out, bnmean, bnstd\n",
    "\n",
    "\n",
    "def forward(X_batch, h1_prev, h2_prev, bnmean1, bnstd1, bnmean2, bnstd2):\n",
    "    batch_size, seq_len = X_batch.shape\n",
    "    h1_prev = h1_prev.repeat(batch_size, 1)\n",
    "    h2_prev = h2_prev.repeat(batch_size, 1)\n",
    "\n",
    "    outs = []\n",
    "    h1s = []\n",
    "    h2s = []\n",
    "\n",
    "    for t in range(seq_len):\n",
    "        X_t = X_batch[:, t]\n",
    "        X_embed = Embedding_vector[X_t]\n",
    "\n",
    "        h1_pre = X_embed @ Wxh1.T + h1_prev @ Whh1.T + bh1.T\n",
    "        h1, mean1, std1 = batch_normalization(h1_pre, bgain1, bnbias1)\n",
    "        h1 = torch.tanh(h1)\n",
    "\n",
    "        h2_pre = h1 @ Wxh2.T + h2_prev @ Whh2.T + bh2.T\n",
    "        h2, mean2, std2 = batch_normalization(h2_pre, bgain2, bnbias2)\n",
    "        h2 = torch.tanh(h2)\n",
    "\n",
    "        out = h2 @ Why.T + by.T\n",
    "\n",
    "        outs.append(out)\n",
    "        h1s.append(h1)\n",
    "        h2s.append(h2)\n",
    "\n",
    "        h1_prev = h1\n",
    "        h2_prev = h2\n",
    "\n",
    "    with torch.no_grad():\n",
    "        bnmean1 = 0.999 * bnmean1 + 0.001 * mean1\n",
    "        bnstd1  = 0.999 * bnstd1  + 0.001 * std1\n",
    "        bnmean2 = 0.999 * bnmean2 + 0.001 * mean2\n",
    "        bnstd2  = 0.999 * bnstd2  + 0.001 * std2\n",
    "\n",
    "    outs = torch.stack(outs, dim=1)\n",
    "    h1s = torch.stack(h1s, dim=1)\n",
    "    h2s = torch.stack(h2s, dim=1)\n",
    "\n",
    "    return outs, h1s, h2s, bnmean1, bnstd1, bnmean2, bnstd2\n",
    "\n",
    "\n",
    "def backward_batch(X_batch, h1_batch, h2_batch, out_batch, y_batch):\n",
    "    dWxh1 = torch.zeros_like(Wxh1)\n",
    "    dWhh1 = torch.zeros_like(Whh1)\n",
    "    dbh1  = torch.zeros_like(bh1)\n",
    "\n",
    "    dWxh2 = torch.zeros_like(Wxh2)\n",
    "    dWhh2 = torch.zeros_like(Whh2)\n",
    "    dbh2  = torch.zeros_like(bh2)\n",
    "\n",
    "    dWhy  = torch.zeros_like(Why)\n",
    "    dby   = torch.zeros_like(by)\n",
    "\n",
    "    dEmbedding = torch.zeros_like(Embedding_vector)\n",
    "\n",
    "    batch_size = X_batch.shape[0]\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        X_seq = X_batch[b]\n",
    "        h1_seq = h1_batch[b]\n",
    "        h2_seq = h2_batch[b]\n",
    "        out_seq = out_batch[b]\n",
    "        target_seq = y_batch[b]\n",
    "\n",
    "        dh1_next = torch.zeros_like(h1_seq[0])\n",
    "        dh2_next = torch.zeros_like(h2_seq[0])\n",
    "\n",
    "        for t in reversed(range(len(X_seq))):\n",
    "            out = out_seq[t]\n",
    "            h1 = h1_seq[t]\n",
    "            h2 = h2_seq[t]\n",
    "            X_index = X_seq[t]\n",
    "            target_index = target_seq if isinstance(target_seq, int) else target_seq[t]\n",
    "\n",
    "            probs = torch.softmax(out, dim=0)\n",
    "            probs[target_index] -= 1\n",
    "            dout = probs\n",
    "\n",
    "            dWhy += dout @ h2.T\n",
    "            dby  += dout\n",
    "\n",
    "            dh2 = Why.T @ dout + dh2_next\n",
    "            dh2_raw = dh2 * (1 - h2**2)\n",
    "\n",
    "            dWxh2 += dh2_raw @ h1.T\n",
    "            dWhh2 += dh2_raw @ (h2_seq[t-1] if t > 0 else torch.zeros_like(h2)).T\n",
    "            dbh2  += dh2_raw\n",
    "\n",
    "            dh1 = Wxh2.T @ dh2_raw + dh1_next\n",
    "            dh1_raw = dh1 * (1 - h1**2)\n",
    "\n",
    "            dWxh1 += dh1_raw @ Embedding_vector[X_index].T\n",
    "            dWhh1 += dh1_raw @ (h1_seq[t-1] if t > 0 else torch.zeros_like(h1)).T\n",
    "            dbh1  += dh1_raw\n",
    "\n",
    "            dEmbedding[X_index] += Wxh1.T @ dh1_raw\n",
    "\n",
    "            dh1_next = Whh1.T @ dh1_raw\n",
    "            dh2_next = Whh2.T @ dh2_raw\n",
    "\n",
    "    return (dWxh1 / batch_size, dWhh1 / batch_size, dbh1 / batch_size,\n",
    "            dWxh2 / batch_size, dWhh2 / batch_size, dbh2 / batch_size,\n",
    "            dWhy / batch_size, dby / batch_size, dEmbedding / batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d244e423",
   "metadata": {},
   "source": [
    "## Model Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e32cdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 2000\n",
    "batch_size = 32\n",
    "\n",
    "bnmean1 = torch.zeros(1, hidden_size)\n",
    "bnstd1  = torch.ones(1, hidden_size)\n",
    "bnmean2 = torch.zeros(1, hidden_size)\n",
    "bnstd2  = torch.ones(1, hidden_size)\n",
    "\n",
    "for i in range(max_steps):\n",
    "    h1_prev = torch.zeros(hidden_size, 1)\n",
    "    h2_prev = torch.zeros(hidden_size, 1)\n",
    "\n",
    "    ix = torch.randint(0, len(X_train), (batch_size,))\n",
    "    Xb, yb = X_train[ix], y_train[ix]\n",
    "\n",
    "    out_batch, h1_batch, h2_batch, bnmean1, bnstd1, bnmean2, bnstd2 = forward(\n",
    "        Xb, h1_prev, h2_prev, bnmean1, bnstd1, bnmean2, bnstd2\n",
    "    )\n",
    "\n",
    "    loss, _ = softmax_cross_entropy(out_batch, yb)\n",
    "\n",
    "    grads = backward_batch(Xb, h1_batch, h2_batch, out_batch, yb)\n",
    "\n",
    "    lr = 0.1 if i < 200 else 0.01\n",
    "\n",
    "    for p, g in zip(parameters, grads):\n",
    "        p.data -= lr * g\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
